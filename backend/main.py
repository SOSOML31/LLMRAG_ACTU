from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from database import SessionLocal, Article
from ollama_handler import ask_ollama
from pydantic import BaseModel
import ollama

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class QueryRequest(BaseModel):
    query: str

@app.post("/ask")
def ask(request: QueryRequest):
    query = request.query

    session = SessionLocal()
    articles = session.query(Article).order_by(Article.created_at.desc()).limit(10).all()
    session.close()

    if not articles:
        raise HTTPException(status_code=404, detail="Aucun article trouv√©.")

    # Correction ici - Assurer que la liste est bien formatt√©e
    context = "\n\n".join([f"{article.title}: {article.content[:500]}..." for article in articles])

    sources = [{"title": article.title, "url": article.url} for article in articles]

    # G√©n√©ration de la r√©ponse avec Ollama
    response = ask_ollama(query, context)

    return {"response": response, "sources": sources}

@app.post("/llm")
def direct_llm_query(request: QueryRequest):
    """Appelle Ollama avec un mod√®le au choix"""
    query = request.query
    model = request.model if hasattr(request, "model") else "mistral"  # Mistral par d√©faut
    print(f"üß† Requ√™te LLM re√ßue : {query} | Mod√®le : {model}")

    # üéØ Cr√©ation du prompt simple
    prompt = f"R√©ponds de mani√®re pr√©cise et d√©taill√©e √† cette question : {query}"

    # ü§ñ Envoi au LLM Ollama avec le mod√®le s√©lectionn√©
    response = ollama.chat(model=model, messages=[{"role": "user", "content": prompt}])

    return {"response": response["message"]["content"]}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=5000)