from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from database import SessionLocal, Article
from ollama_handler import ask_ollama
from pydantic import BaseModel
import ollama

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mod√®le de requ√™te attendu pour les endpoints POST
class QueryRequest(BaseModel):
    query: str

@app.post("/ask")
def ask(request: QueryRequest):
    query = request.query  # R√©cup√©ration de la requ√™te utilisateur
    session = SessionLocal()  # Ouverture d'une session de base de donn√©es
    # R√©cup√©ration des 10 articles les plus r√©cents
    articles = session.query(Article).order_by(Article.created_at.desc()).limit(10).all()
    session.close()  # Fermeture de la session

    # V√©rification si des articles sont disponibles
    if not articles:
        raise HTTPException(status_code=404, detail="Aucun article trouv√©.")

    # Formatage du contexte √† partir des articles r√©cup√©r√©s
    context = "\n\n".join([f"{article.title}: {article.content[:500]}..." for article in articles])

    # Cr√©ation d'une liste des sources pour la r√©ponse
    sources = [{"title": article.title, "url": article.url} for article in articles]

    # Appel au mod√®le Ollama avec la requ√™te et le contexte
    response = ask_ollama(query, context)

    return {"response": response, "sources": sources}  # Retourne la r√©ponse du mod√®le et les sources utilis√©es


@app.post("/llm")
def direct_llm_query(request: QueryRequest):
    """Appelle directement Ollama avec un mod√®le au choix."""
    query = request.query  # R√©cup√©ration de la requ√™te utilisateur
    model = request.model if hasattr(request, "model") else "mistral"  # S√©lection du mod√®le, Mistral par d√©faut

    print(f"üß† Requ√™te LLM re√ßue : {query} | Mod√®le : {model}")  # Log pour le suivi

    prompt = f"R√©ponds de mani√®re pr√©cise et d√©taill√©e √† cette question : {query}"

    response = ollama.chat(model=model, messages=[{"role": "user", "content": prompt}])

    return {"response": response["message"]["content"]}  # Retourne la r√©ponse g√©n√©r√©e


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=5000)