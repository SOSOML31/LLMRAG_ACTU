import feedparser
import requests
from bs4 import BeautifulSoup
from database import SessionLocal, Article
##############################################################################################################################################################################
##############################################################################################################################################################################
##############################################################################################################################################################################
#        üéØ Ce script r√©cup√®re des articles depuis un flux RSS et les stocke dans PostgreSQL et ChromaDB.                         ###########################################
##############################################################################################################################################################################
##############################################################################################################################################################################
##############################################################################################################################################################################
##############################################################################################################################################################################

def fetch_rss_articles(rss_url):
    """
    üìå Cette fonction :
    1Ô∏è‚É£ T√©l√©charge le flux RSS depuis une URL donn√©e.
    2Ô∏è‚É£ Extrait le contenu des articles en HTML et le convertit en texte.
    3Ô∏è‚É£ V√©rifie si l'article existe d√©j√† en base de donn√©es (PostgreSQL).
    4Ô∏è‚É£ Si l'article est nouveau, il est ajout√© √† PostgreSQL.
    """
    session = SessionLocal()
    feed = feedparser.parse(rss_url)

    for entry in feed.entries:  # Boucle sur chaque article du flux RSS

        # R√©cup√©rer le contenu de l'article via son lien
        response = requests.get(entry.link)
        soup = BeautifulSoup(response.text, "html.parser")  # Parser le HTML de l'article

        # Extraire le texte des balises <p> pour obtenir uniquement le contenu
        text = " ".join([p.text for p in soup.find_all("p")])

        # V√©rifier si l'article est d√©j√† en base (on v√©rifie via l'URL unique)
        existing_article = session.query(Article).filter_by(url=entry.link).first()

        if not existing_article:  # Si l'article n'est PAS encore dans la base
            # 4Ô∏è‚É£ Cr√©er un nouvel objet Article pour le stocker dans PostgreSQL
            new_article = Article(title=entry.title, url=entry.link, content=text)
            session.add(new_article)  # Ajouter l'article √† la session SQL
            session.commit()  # Enregistrer dans la base
            print(f"‚úÖ Article ajout√©: {entry.title}")  # Affichage en console
        else:
            print(f"‚ö†Ô∏è Article d√©j√† pr√©sent en base: {entry.title}")  # D√©tection des doublons

    session.close()  # Fermer la connexion PostgreSQL
    print("‚úÖ Tous les articles ont √©t√© trait√©s.")  # Indiquer la fin du processus

fetch_rss_articles("https://www.france24.com/fr/rss")